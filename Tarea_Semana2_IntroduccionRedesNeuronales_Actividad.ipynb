{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Tarea_Semana2_IntroduccionRedesNeuronales_Actividad.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjb4EWrMbP1r",
        "colab_type": "text"
      },
      "source": [
        "## MIIA-4203 MODELOS AVANZADOS PARA ANÁLISIS DE DATOS II\n",
        "\n",
        "\n",
        "# Introducción a las redes neuronales\n",
        "\n",
        "## Actividad 2\n",
        "\n",
        "### Profesor: Camilo Franco (c.franco31@uniandes.edu.co)\n",
        "\n",
        "\n",
        "\n",
        "En esta actividad vamos a estudiar una primera aproximación a los modelos de redes neuronales, utilizando como base el modelo de regresión logística.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vqVVYz0s2I1",
        "colab_type": "text"
      },
      "source": [
        "**Presentado por:**\n",
        "\n",
        "**Alexander Hernández Páez: 200920588**\n",
        "\n",
        "**Juan David Cortés: 201728568**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMgiTNyjbP1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Algunos paquetes iniciales que vamos a utilizar\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e11Vr58qbP1v",
        "colab_type": "text"
      },
      "source": [
        "## 1. Problema de clasificación: riesgo de default\n",
        "\n",
        "Examinemos los datos con lo cuales ya estamos familiarizados:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bqncMt9bP1v",
        "colab_type": "text"
      },
      "source": [
        "https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk2ozrI3bP1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "credit_1 = pd.read_csv(\"germancredit.csv\")\n",
        "credit_1 = pd.get_dummies(credit_1, columns=['checkingstatus1','history','purpose','savings',\n",
        "                                   'employ','status','others','property','otherplans','housing','job','tele', \n",
        "                                   'foreign'], prefix = ['checkingstatus1','history','purpose','savings',\n",
        "                                   'employ','status','others','property','otherplans','housing','job','tele', \n",
        "                                   'foreign'])\n",
        "X = credit_1.iloc[:, 1:62]\n",
        "Y = credit_1.iloc[:, 0]\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGdb1BbZbP1y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6997ec40-39da-4b27-b1c5-6c610d56b2cf"
      },
      "source": [
        "CE_x, CP_x, CE_y, CP_y = model_selection.train_test_split(X, Y, test_size=0.4, random_state=42, stratify=Y)\n",
        "print(\"Tamaño de CE, CP: \", CE_y.shape, CP_y.shape)\n",
        "print(\"Observaciones de la clase positiva en entrenamiento: \" +str(sum(CE_y)) +\" y en prueba: \" +str(sum(CP_y)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tamaño de CE, CP:  (600,) (400,)\n",
            "Observaciones de la clase positiva en entrenamiento: 180 y en prueba: 120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVjHxzzobP11",
        "colab_type": "text"
      },
      "source": [
        "## 2. Construcción de una neurona Sigmoide\n",
        "\n",
        "Una neurona Sigmoide puede ser vista como un perceptrón *suavizado* que recibe una señal y entonces se activa. Al activarse, transforma la señal para entender mejor el mensaje. Esta transformación la ejecuta a partir de la fucnión Sigmoide.\n",
        "\n",
        "Si tomamos la señal como un conjunto de datos de entrada y el mensaje como la predicción de un valor, la función de activación jugará el papel de transformadora de los datos de entrada en aquello que se quiere entender/predecir, que además replica un modelo logit con la función de activación sigmoide.\n",
        "\n",
        "A continuación construiremos un clasificador de regresión logística bajo la perspectiva de una red neuronal, estudiando la arquitectura general de un algoritmo de aprendizaje. De esta manera, necesitaremos incluir la inicialización de los parámetros, el cálculo de la función de coste y su gradiente, y utilizar un algoritmo de optimización como por ejemplo el descenso en la dirección del gradiente (GD)\n",
        "\n",
        "**Formulación del algoritmo**:\n",
        "\n",
        "Para un ejemplo $x^{(i)}$:\n",
        "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
        "$$\\hat{y}^{(i)} = a^{(i)} = sigmoide(z^{(i)})\\tag{2}$$ \n",
        "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
        "\n",
        "El coste se calcula sumando sobre todos los ejemplos de entrenamiento:\n",
        "$$ L = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{4}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0dmBO5dbP11",
        "colab_type": "text"
      },
      "source": [
        "### Construimos las partes del algoritmo  \n",
        "\n",
        "- Inicializar los parámetros del modelo\n",
        "- Bucle:\n",
        "    - Calcular la pérdida actual (propagación hacia delante)\n",
        "    - Calcular el gradiente actual (retro-propagación)\n",
        "    - Actualizar los parámetros (descenso en la dirección del gradiente)\n",
        "\n",
        "\n",
        "### Ejercicio 2.1\n",
        "Implemente la funcion `sigmoide()` $$\\sigma( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$$ Para ello puede utilizar np.exp()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgmty1zObP12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoide(z):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "    z: Un escalar o arreglo numpy de cualquier tamaño\n",
        "    Output:\n",
        "    s: sigmoid(z)\n",
        "    \"\"\"\n",
        "    s = 1/(1+np.exp(-z))\n",
        "\n",
        "    return s"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBHPvsltbP14",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "9fa5f90d-255a-48be-95d1-d6ae95bc965c"
      },
      "source": [
        "print (\"sigmoide([99,1,0,-1,-99]) = \" + str(sigmoide(np.array([99,1,0,-1,-99]))))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sigmoide([99,1,0,-1,-99]) = [1.00000000e+00 7.31058579e-01 5.00000000e-01 2.68941421e-01\n",
            " 1.01122149e-43]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZizNlGzbP16",
        "colab_type": "text"
      },
      "source": [
        "**Salida esperada**: \n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "<td> sigmoide([99,1,0,-1,-99])    = </td>\n",
        "<td> [ 1.00000000e+00 7.31058579e-01 5.00000000e-01 2.68941421e-01\n",
        " 1.01122149e-43] </td> \n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgXSbgthbP16",
        "colab_type": "text"
      },
      "source": [
        "### Ejercicio 2.2 \n",
        "\n",
        "Debemos inicializar los parámetros a cero. Puede utilizar la funcion np.zeros(), apoyandose en la documentación de la biblioteca Numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdNoIC6dbP17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inicializa_ceros(dim,semilla):\n",
        "    \"\"\"\n",
        "    Esta función crea un vector de ceros de dimensión (dim, 1) para w e inicializa b a 0.\n",
        "    Input:\n",
        "    dim: tamaño del vector w (número de parámetros para este caso)\n",
        "    Output:\n",
        "    w: vector inicializado de tamaño (dim, 1)\n",
        "    b: escalar inicializado (corresponde con el sesgo)\n",
        "    \"\"\"\n",
        "    if semilla!=0:\n",
        "      np.random.seed(semilla) # we set up a seed so that your output matches ours although the initialization is random.\n",
        "      w = np.random.randn(dim,1) * 0.01\n",
        "      b = 0\n",
        "\n",
        "    if semilla==0:\n",
        "      w = np.zeros([dim,1])\n",
        "      b = 0\n",
        "\n",
        "    assert(w.shape == (dim, 1))\n",
        "    assert(isinstance(b, float) or isinstance(b, int))\n",
        "    \n",
        "    return w, b"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvXy2HoIbP19",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "1a99f356-3fa3-4dfb-b480-87c5b799ba02"
      },
      "source": [
        "dim = 6\n",
        "w, b = inicializa_ceros(dim,semilla=0)\n",
        "print (\"w = \" + str(w))\n",
        "print (\"b = \" + str(b))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w = [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "b = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWfcaDa6bP1_",
        "colab_type": "text"
      },
      "source": [
        "**Salida esperada**: \n",
        "\n",
        "\n",
        "<table style=\"width:35%\">\n",
        "<tr>\n",
        "<td>   w   </td>\n",
        "<td> [[0.]\n",
        " [0.]\n",
        " [0.]\n",
        " [0.]\n",
        " [0.]\n",
        " [0.]] </td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>   b   </td>\n",
        "<td> 0 </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8smvv2PbP2B",
        "colab_type": "text"
      },
      "source": [
        "### Ejercicio 2.3 \n",
        "#### Propagación hacia delante y hacia atrás\n",
        "\n",
        "Una vez los estimadores están inicializados, se pueden implementar los pasos de propagación hacia \"delante\" y hacia \"atrás\" para el aprendizaje automático. \n",
        "\n",
        "La propagación hacia delante consiste en calcular la función de activación sigmoide sobre la combinacón lineal de los patrones y los coeficientes inciales. \n",
        "\n",
        "Luego la propagación hacia atrás, o *retro-propagación*, es el paso más importante, donde utilizamos el gradiente de la función del error o de pérdida para actualizar los coeficientes. \n",
        "\n",
        "Este procedimiento se repite iterativamente replicando el procediemiento de descenso en la dirección del gradiente o *Gradient Descent* (GD).\n",
        "\n",
        "A continuación implemente la función `propaga()` que calcula la función de coste y su gradiente.\n",
        "\n",
        "**Ayuda**:\n",
        "\n",
        "Propagación hacia delante:\n",
        "- Se tiene $X$\n",
        "- Se calcula $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
        "- Se calcula la función de coste/pérdida: $L = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
        "\n",
        "Para la retro-propagación, tenemos que calcular la derivada parcial de *L* con respecto a nuestros coeficientes $(w,b)$:  \n",
        "\n",
        "$$ \\frac{\\partial L}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{5}$$\n",
        "$$ \\frac{\\partial L}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{6}$$\n",
        "\n",
        "*Nota:* Para el cálculo de estas derivadas debemos hacer uso de la regla de la cadena. \n",
        "\n",
        "Esto es, dado $Z=w^T X + b$, se tiene que $$\\frac{\\partial L}{\\partial Z} = \\frac{\\partial L}{\\partial A} \\frac{\\partial A}{\\partial Z} = \\bigg(\\frac{-Y}{A}+\\frac{1-Y}{1-A}\\bigg) (A \\cdot (1-A)) $$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_PkAP0VbP2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def propaga(w, b, X, Y):\n",
        "    \"\"\"\n",
        "    Implemente la función de coste y su gradiente para la propagación\n",
        "    Input:\n",
        "    w: pesos, un arreglo numpy \n",
        "    b: sesgo, un escalar\n",
        "    X: datos de entrada\n",
        "    Y: vector de etiquetas \n",
        "    Output:\n",
        "    coste: coste negativo de log-verosimilitud para la regresión logística\n",
        "    dw: gradiente de la pérdida con respecto a w, con las mismas dimensiones que w\n",
        "    db: gradiente de la pérdida con respecto a b, con las mismas dimensiones que b\n",
        "    \n",
        "    (Sugerencia: utilice las funciones np.log(), np.dot()\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    \n",
        "    A = sigmoide(np.dot(w.T, X) + b)                    # compute la activación\n",
        "    coste = -(1/m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))                # compute el coste\n",
        "\n",
        "    dw = (1/m)*np.dot(X,(A-Y).T) \n",
        "    db = (1/m)*np.sum(A-Y)\n",
        "\n",
        "    assert(dw.shape == w.shape)\n",
        "    assert(db.dtype == float)\n",
        "    coste = np.squeeze(coste)\n",
        "    assert(coste.shape == ())\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return grads, coste"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOniEvItbP2D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "a255032a-d77e-4856-92de-66bfcfd8037e"
      },
      "source": [
        "w, b, X, Y = np.array([[0.1],[0.1]]), 0.5, np.array([[66.,99.,-33.],[32.,55.,-2.1]]), np.array([[0,0,1]])\n",
        "grads, coste = propaga(w, b, X, Y)\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))\n",
        "print (\"coste = \" + str(coste))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dw = [[65.48251839]\n",
            " [29.66675568]]\n",
            "db = 0.348980796447886\n",
            "coste = 9.752716367426284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u26yFCLIbP2G",
        "colab_type": "text"
      },
      "source": [
        "**Salida esperada**: \n",
        "\n",
        "<table style=\"width:50%\">\n",
        "<tr>\n",
        "<td>   dw   </td>\n",
        "<td> [[65.48251839]\n",
        " [29.66675568]]</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>   db   </td>\n",
        "<td> 0.348980796447886 </td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>   cost   </td>\n",
        "<td> 9.752716367426284 </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHioVMbFbP2G",
        "colab_type": "text"
      },
      "source": [
        "### Ejercicio 2.4 \n",
        "#### Optimización\n",
        "\n",
        "- Se tienen los parámetros inicializados.\n",
        "- También se tiene el código para calcular la función de coste y su gradiente.\n",
        "- Ahora se quieren actualizar los parámetros utilizando el GD.\n",
        "\n",
        "Escriba la función de optimización para aprender $w$ y $b$ minimizando la función de coste $L$. \n",
        "\n",
        "Para un parámetro $\\theta$, la regla de actualización es $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, donde $\\alpha$ es la tasa de aprendizaje."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aInjxfHSbP2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimiza(w, b, X, Y, num_iter, tasa, print_cost):\n",
        "    \"\"\"\n",
        "    Esta función optimiza w y b implementando el algoritmo de GD\n",
        "    Input:\n",
        "    w: pesos, un arreglo numpy \n",
        "    b: sesgo, un escalar\n",
        "    X: datos de entrada\n",
        "    Y: vector de etiquetas \n",
        "    num_iter: número de iteracionespara el bucle de optimización\n",
        "    tasa: tasa de aprendizaje para la regla de actualización del GD\n",
        "    print_cost: True para imprimir la pérdida cada 100 iteraciones\n",
        "    Output:\n",
        "    params: diccionario con los pesos w y el sesgo b\n",
        "    grads: diccionario con los gradientes de los pesos y el sesgo con respecto a la función de pérdida\n",
        "    costes: lista de todos los costes calculados durante la optimización, usados para graficar la curva de aprendizaje.\n",
        "    \n",
        "    Sugerencia: puede escribir dos pasos e iterar sobre ellos:\n",
        "        1) Calcule el coste y el gradiente de los parámetros actuales. Use propaga().\n",
        "        2) Actualize los parámetros usando la regla del GD para w y b.\n",
        "    \"\"\"\n",
        "    \n",
        "    costes = []\n",
        "    \n",
        "    for i in range(num_iter):\n",
        "        \n",
        "        \n",
        "        # Computación del coste y el gradiente \n",
        "        grads, coste = propaga(w, b, X, Y)\n",
        "        \n",
        "        # Recupere las derivadas de grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "        \n",
        "        # Actualize la regla \n",
        "        w = w - tasa * dw\n",
        "        b = b - tasa * db\n",
        "        \n",
        "        # Guarde los costes\n",
        "        if i % 100 == 0:\n",
        "            costes.append(coste)\n",
        "        \n",
        "        # Se muestra el coste cada 100 iteraciones de entrenamiento\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Coste tras la iteración %i: %f\" %(i, coste))\n",
        "    \n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return params, grads, costes"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEw3lvaibP2J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "6f0662a5-a08e-4d25-f57c-1f940f2a1113"
      },
      "source": [
        "params, grads, costes = optimiza(w, b, X, Y, num_iter= 10, tasa = 0.001, print_cost = False)\n",
        "\n",
        "print (\"w = \" + str(params[\"w\"]))\n",
        "print (\"b = \" + str(params[\"b\"]))\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w = [[-0.07262234]\n",
            " [ 0.02112647]]\n",
            "b = 0.49898148713402446\n",
            "dw = [[1.42076721]\n",
            " [0.43496446]]\n",
            "db = -0.007821662502973652\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWvhYW3_bP2L",
        "colab_type": "text"
      },
      "source": [
        "**Salida esperada**:  \n",
        "\n",
        "<table style=\"width:40%\">\n",
        "<tr>\n",
        "<td> w </td>\n",
        "<td>[[-0.07262234]\n",
        " [ 0.02112647]] </td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td> b </td>\n",
        "<td> 0.49898148713402446 </td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td> dw </td>\n",
        "<td> [[1.42076721]\n",
        " [0.43496446]] </td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td> db </td>\n",
        "<td> -0.007821662502973652 </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7YrJ1W1bP2L",
        "colab_type": "text"
      },
      "source": [
        "### Ejercicio 2.5\n",
        "\n",
        "La función anterior aprende los parámetros w y b, que se pueden usar para predecir sobre el conjunto de datos X. \n",
        "\n",
        "Hay dos pasos para calcular las predicciones:\n",
        "\n",
        "1. Calcular $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
        "\n",
        "2. Converir a 0 las entradas de $a$ (si la activación es <= 0.5) o 1 (si la activación es > 0.5), guarde las predicciones en un vector `Y_pred`.  \n",
        "\n",
        "Ahora implemente la función `pred()`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5hlzWjtbP2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pred(w, b, X):\n",
        "    '''\n",
        "    Prediga si una etiqueta es 0 o 1 usando los parámetros de regresión logística aprendidos (w, b)\n",
        "    Input:\n",
        "    w: pesos, un arreglo numpy \n",
        "    b: sesgo, un escalar\n",
        "    X: datos de entrada\n",
        "    Output:\n",
        "    Y_pred: vector con todas las predicciones (0/1) para los ejemplos en X\n",
        "    '''\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    Y_pred = np.zeros((1,m))\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "    \n",
        "    # Compute el vector \"A\" prediciendo las probabilidades de que la imagen contenga un frailejon\n",
        "    A = sigmoide(np.dot(w.T, X) + b)\n",
        "    \n",
        "    for i in range(A.shape[1]):\n",
        "        \n",
        "        # Convierta las probabilidades A[0,i] a predicciones p[0,i]\n",
        "        Y_pred[0,i] = round(A[0,i],0)\n",
        "    \n",
        "    assert(Y_pred.shape == (1, m))\n",
        "    \n",
        "    return Y_pred"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rxbnc09IbP2N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "cf01e1b0-587f-4a55-d7fa-f9458b08f796"
      },
      "source": [
        "w = np.array([[0.12],[0.23]])\n",
        "b = -0.09\n",
        "X = np.array([[3.1,-2.9,0.2],[1.9,1.8,-0.09]])\n",
        "print (\"predicciones = \" + str(pred(w, b, X)))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicciones = [[1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uOJ4OQIbP2P",
        "colab_type": "text"
      },
      "source": [
        "**Salida esperada**: \n",
        "\n",
        "<table style=\"width:40%\">\n",
        "<tr>\n",
        "<td> predicciones   </td>\n",
        "<td>[[ 1.  0.  0.]]  </td>  \n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA4ea17EbP2P",
        "colab_type": "text"
      },
      "source": [
        "### Ejercicio 2.6\n",
        "#### Combine todas las funciones \n",
        "\n",
        "Ahora juntemos todos los bloques que ha programado arriba.\n",
        "\n",
        "Implemente la función del modelo \"madre\". Use la siguiente notación:\n",
        "    - YP_pred para las predicciones sobre el conjunto de prueba\n",
        "    - YE_pred para las predicciones sobre el conjunto de entrenamiento\n",
        "    - w, costes, grads para las salidas de optimiza()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Df5zf5OdbP2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def modelo(CE_x, CP_x, CE_y, CP_y, num_iter, tasa, print_cost,semilla):\n",
        "    \"\"\"\n",
        "    Construye el modelo de regresión logística llamando las funciones implementadas anteriormente\n",
        "    Output:\n",
        "    d: diccionario con la información sobre el modelo.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Inicialice los parametros con ceros \n",
        "    w, b = inicializa_ceros(CE_x.shape[0],semilla)\n",
        "\n",
        "    # Descenso en la dirección del gradiente (GD) \n",
        "    params, grads, costes = optimiza(w, b, CE_x, CE_y, num_iter, tasa, print_cost)\n",
        "    \n",
        "    # Recupere los parámetros w y b del diccionario \"params\" ##\n",
        "    w = params[\"w\"]\n",
        "    b = params[\"b\"]\n",
        "    \n",
        "    # Prediga los ejemplos de prueba y entrenamiento (≈ 2 líneas de código)\n",
        "    YP_pred = pred(w, b, CP_x)\n",
        "    YE_pred = pred(w, b, CE_x)\n",
        "\n",
        "    # Imprima los errores de entrenamiento y prueba\n",
        "    print(\"Accuracy de entrenamiento: {} %\".format(100 - np.mean(np.abs(YE_pred - CE_y)) * 100))\n",
        "    print(\"Accuracy de prueba: {} %\".format(100 - np.mean(np.abs(YP_pred - CP_y)) * 100))\n",
        "\n",
        "    \n",
        "    d = {\"Costes\": costes,\n",
        "         \"Prediccion_prueba\": YP_pred, \n",
        "         \"Prediccion_entrenamiento\" : YE_pred, \n",
        "         \"w\" : w, \n",
        "         \"b\" : b,\n",
        "         \"Tasa de aprendizaje\" : tasa,\n",
        "         \"Numero de iteraciones\": num_iter}\n",
        "    \n",
        "    return d"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp1FDyQ8bP2R",
        "colab_type": "text"
      },
      "source": [
        "### Pregunta 2.7\n",
        "\n",
        "De qué dimensiones deben ser las matrices con los datos de entrada y de salida?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbwrG44JbP2S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "df3af736-4b43-4c65-c581-0ba5181ebac7"
      },
      "source": [
        "# Podemos re-configurar las matrices de la siguiente forma:\n",
        "CE_x2 = CE_x.T\n",
        "CP_x2 = CP_x.T\n",
        "CE_y2 = np.array(CE_y)[np.newaxis]\n",
        "CP_y2 = np.array(CP_y)[np.newaxis]\n",
        "\n",
        "print(CE_x2.shape, CE_y2.shape)\n",
        "print(CP_x2.shape, CP_y2.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(61, 600) (1, 600)\n",
            "(61, 400) (1, 400)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAf2zi1Ip0FF",
        "colab_type": "text"
      },
      "source": [
        "**Los datos de entrenamiento tienen 600 ejemplos y 61 explicativas, mientras que los datos de prueba contienen 400 ejemplos y las mismas 61 explicativas. La respuesta en entrenamiento constituye un vector de dimension 600 y 400 en entrenamiento y prueba, respectivamente.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4iVTHNAbP2U",
        "colab_type": "text"
      },
      "source": [
        "Ahora, ejecute la siguiente celda para entrenar el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kWzJ8WWbP2U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "da4cb0f8-696b-42f0-ff00-d223761e8c35"
      },
      "source": [
        "d = modelo(CE_x2, CP_x2, CE_y2, CP_y2, num_iter = 1000, tasa = 1e-6, print_cost = True,semilla=0)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Coste tras la iteración 0: 0.693147\n",
            "Coste tras la iteración 100: 0.678157\n",
            "Coste tras la iteración 200: 0.677031\n",
            "Coste tras la iteración 300: 0.675938\n",
            "Coste tras la iteración 400: 0.674877\n",
            "Coste tras la iteración 500: 0.673846\n",
            "Coste tras la iteración 600: 0.672844\n",
            "Coste tras la iteración 700: 0.671871\n",
            "Coste tras la iteración 800: 0.670925\n",
            "Coste tras la iteración 900: 0.670005\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmdaEheEbP2W",
        "colab_type": "text"
      },
      "source": [
        "**Salida esperada**: \n",
        "\n",
        "<table style=\"width:50%\"> \n",
        "<tr>\n",
        "<td> Coste tras la iteración 0   </td> \n",
        "<td> 0.693147 </td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td> <center> $\\vdots$ </center> </td> \n",
        "<td> <center> $\\vdots$ </center> </td> \n",
        "</tr>  \n",
        "<tr>\n",
        "<td> Precisión de entrenamiento  </td> \n",
        "<td> 70.0 % </td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td> Precisión de prueba </td> \n",
        "<td> 70.0 % </td>\n",
        "</tr>\n",
        "</table> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaZ1CR-qbP2X",
        "colab_type": "text"
      },
      "source": [
        "regresion logistica. También podemos observar que el error de prueba es igual al de entrenamiento. Este resultado sugiere que el modelo aprende segun entrenamiento, y generaliza de igual forma sobre los observaciones nuevas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBX31Fp6bP2X",
        "colab_type": "text"
      },
      "source": [
        "Grafiquemos la función de pérdida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG_t0Gu2bP2Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "e742f3d2-e551-4629-feb0-9a1a70cb8f48"
      },
      "source": [
        "# Gráfica de la curva de aprendizaje (con costes)\n",
        "costes = np.squeeze(d['Costes'])\n",
        "plt.plot(costes)\n",
        "plt.ylabel('coste')\n",
        "plt.xlabel('iteraciones (en cientos)')\n",
        "plt.title(\"Tasa de aprendizaje =\" + str(d[\"Tasa de aprendizaje\"]))\n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KayUmsPbP2a",
        "colab_type": "text"
      },
      "source": [
        "**Interpretación**:\n",
        "Se puede ver el coste decreciendo, demostrando que los parámetros están siendo aprendidos. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT-Zmgk-bP2a",
        "colab_type": "text"
      },
      "source": [
        "Ya tenemos un primer modelo de clasificación. Ahora examinemos distintos valores para la tasa de aprendizaje $\\alpha$. \n",
        "\n",
        "#### Selección de la tasa de aprendizaje ####\n",
        "\n",
        "Para que el método del GD funcione de manera adecuada, se debe elegir la tasa de aprendiazaje de manera acertada. Esta tasa $\\alpha$  determina qué tan rápido se actualizan los parámetros. Si la tasa es muy grande se puede \"sobrepasar\" el valor óptimo. Y de manera similar, si es muy pequeña se van a necesitar muchas iteraciones para converger a los mejores valores. Por ello la importancia de tener una tase de aprensizaje bien afinada.  \n",
        "\n",
        "Ahora, comparemos la curva de aprendizaje de nuestro modelo con distintas elecciones para $\\alpha$. Ejecute el código abajo. También puede intentar con valores distintos a los tres que estamos utilizando abajo para `tasas` y analize los resultados.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZJZe98AbP2b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "c0a2f73f-e7e9-459c-c25b-58bb45479834"
      },
      "source": [
        "tasas = [1e-4, 1e-6, 1e-10, 2e-20]\n",
        "modelos = {}\n",
        "for i in tasas:\n",
        "    print (\"La tasa de aprendizaje es: \" + str(i))\n",
        "    modelos[str(i)] = modelo(CE_x2, CP_x2, CE_y2, CP_y2, num_iter = 2000, tasa = i, print_cost = False,semilla=0)\n",
        "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
        "\n",
        "for i in tasas:\n",
        "    plt.plot(np.squeeze(modelos[str(i)][\"Costes\"]), label= str(modelos[str(i)][\"Tasa de aprendizaje\"]))\n",
        "\n",
        "plt.ylabel('coste')\n",
        "plt.xlabel('iteraciones (en cientos)')\n",
        "\n",
        "legend = plt.legend(loc='upper center', shadow=True)\n",
        "frame = legend.get_frame()\n",
        "frame.set_facecolor('0.90')\n",
        "plt.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "La tasa de aprendizaje es: 0.0001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in exp\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy de entrenamiento: 33.0 %\n",
            "Accuracy de prueba: 32.75 %\n",
            "\n",
            "-------------------------------------------------------\n",
            "\n",
            "La tasa de aprendizaje es: 1e-06\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "\n",
            "-------------------------------------------------------\n",
            "\n",
            "La tasa de aprendizaje es: 1e-10\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "\n",
            "-------------------------------------------------------\n",
            "\n",
            "La tasa de aprendizaje es: 2e-20\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "\n",
            "-------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9y4PKp1qfdR",
        "colab_type": "text"
      },
      "source": [
        "**Con la tasa igual a 1e-06 la función de coste minimizá de manera más óptima. Por lo anterior, se utilizaria esta tasa para mejorar el desempeño del modelo.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC1VH15nbP2c",
        "colab_type": "text"
      },
      "source": [
        "### Pregunta 2.8\n",
        "\n",
        "Analice los resultados, con cuál tasa de aprendizaje intentaría mejorar el desempeño del modelo?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv0E67P7Om_1",
        "colab_type": "text"
      },
      "source": [
        "**Con la tasa igual a 1e-06 la función de coste minimiza de manera más óptima. Por lo anterior, se utilizaría esta tasa para mejorar el desempeño del modelo.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubvoTvtPbP2e",
        "colab_type": "text"
      },
      "source": [
        "## 3. Comparacion con la implementación tradicional de regresión logística"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDc91GDvbP2f",
        "colab_type": "text"
      },
      "source": [
        "A continuación ajustamos el modelo logístico y lo probamos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVol0CUHbP2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logT = LogisticRegression(penalty='none', max_iter=1500)\n",
        "logT.fit(CE_x, CE_y)\n",
        "y_tr = logT.predict(CE_x)\n",
        "y_pred = logT.predict(CP_x)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMhvWY2wbP2h",
        "colab_type": "text"
      },
      "source": [
        "Examinemos los coeficientes del modelo de la neurona sigmoide y su desviación con respecto a la estimación tradicional de regresion logistica."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHlCdiGnbP2h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "outputId": "8d9ff1ff-baee-4ac6-c9e1-9bc4d274fd97"
      },
      "source": [
        "from astropy.table import QTable, Table, Column\n",
        "\n",
        "Tabla =  Table([logT.coef_.T, d['w'], CE_x.T], names=(\"Regresion logistica\", \"Neurona sigmoide\", \"Diferencia\"))\n",
        "Tabla"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<i>Table length=61</i>\n",
              "<table id=\"table140062133986248\" class=\"table-striped table-bordered table-condensed\">\n",
              "<thead><tr><th>Regresion logistica [1]</th><th>Neurona sigmoide [1]</th><th>Diferencia [600]</th></tr></thead>\n",
              "<thead><tr><th>float64</th><th>float64</th><th>int64</th></tr></thead>\n",
              "<tr><td>0.022896132865997627</td><td>-6.293083905403856e-05</td><td>18 .. 15</td></tr>\n",
              "<tr><td>0.00010899827650279354</td><td>-1.631136907400603e-05</td><td>12976 .. 3029</td></tr>\n",
              "<tr><td>0.3244034132126813</td><td>-0.00015135061139790666</td><td>3 .. 2</td></tr>\n",
              "<tr><td>0.07379703610118447</td><td>-0.00016794413611101247</td><td>4 .. 2</td></tr>\n",
              "<tr><td>-0.008265241352952623</td><td>-0.0023681544427456127</td><td>38 .. 33</td></tr>\n",
              "<tr><td>0.2400698035951681</td><td>-8.574184624564529e-05</td><td>1 .. 1</td></tr>\n",
              "<tr><td>0.19145797108504312</td><td>-6.586234732681293e-05</td><td>1 .. 1</td></tr>\n",
              "<tr><td>0.5511262709951815</td><td>2.9167410750735905e-05</td><td>0 .. 0</td></tr>\n",
              "<tr><td>0.1186436507074439</td><td>8.42971128730068e-06</td><td>1 .. 0</td></tr>\n",
              "<tr><td>-0.3686183644878829</td><td>-5.488449210699085e-06</td><td>0 .. 0</td></tr>\n",
              "<tr><td>...</td><td>...</td><td>...</td></tr>\n",
              "<tr><td>-0.4465028463512348</td><td>-7.701646645973132e-05</td><td>0 .. 1</td></tr>\n",
              "<tr><td>-0.7581525894057772</td><td>3.986121808875351e-06</td><td>1 .. 0</td></tr>\n",
              "<tr><td>-0.07483717811319153</td><td>1.991680499858925e-06</td><td>0 .. 0</td></tr>\n",
              "<tr><td>-0.12903807693367303</td><td>-1.5375843769924963e-05</td><td>0 .. 0</td></tr>\n",
              "<tr><td>-0.09417833038237476</td><td>-4.5352990748194025e-05</td><td>0 .. 1</td></tr>\n",
              "<tr><td>-0.4248704985180795</td><td>-7.254015435877613e-07</td><td>1 .. 0</td></tr>\n",
              "<tr><td>-0.21158903201378262</td><td>-2.70263683647599e-05</td><td>0 .. 1</td></tr>\n",
              "<tr><td>-0.5113350519409068</td><td>-3.2436187197088e-05</td><td>1 .. 0</td></tr>\n",
              "<tr><td>0.355004333231315</td><td>-4.6792409931105244e-05</td><td>1 .. 1</td></tr>\n",
              "<tr><td>-1.077928417176585</td><td>-1.2670145630742544e-05</td><td>0 .. 0</td></tr>\n",
              "</table>"
            ],
            "text/plain": [
              "<Table length=61>\n",
              "Regresion logistica [1]   Neurona sigmoide [1]  Diferencia [600]\n",
              "        float64                 float64              int64      \n",
              "----------------------- ----------------------- ----------------\n",
              "   0.022896132865997627  -6.293083905403856e-05         18 .. 15\n",
              " 0.00010899827650279354  -1.631136907400603e-05    12976 .. 3029\n",
              "     0.3244034132126813 -0.00015135061139790666           3 .. 2\n",
              "    0.07379703610118447 -0.00016794413611101247           4 .. 2\n",
              "  -0.008265241352952623  -0.0023681544427456127         38 .. 33\n",
              "     0.2400698035951681  -8.574184624564529e-05           1 .. 1\n",
              "    0.19145797108504312  -6.586234732681293e-05           1 .. 1\n",
              "     0.5511262709951815  2.9167410750735905e-05           0 .. 0\n",
              "     0.1186436507074439    8.42971128730068e-06           1 .. 0\n",
              "    -0.3686183644878829  -5.488449210699085e-06           0 .. 0\n",
              "                    ...                     ...              ...\n",
              "    -0.4465028463512348  -7.701646645973132e-05           0 .. 1\n",
              "    -0.7581525894057772   3.986121808875351e-06           1 .. 0\n",
              "   -0.07483717811319153   1.991680499858925e-06           0 .. 0\n",
              "   -0.12903807693367303 -1.5375843769924963e-05           0 .. 0\n",
              "   -0.09417833038237476 -4.5352990748194025e-05           0 .. 1\n",
              "    -0.4248704985180795  -7.254015435877613e-07           1 .. 0\n",
              "   -0.21158903201378262   -2.70263683647599e-05           0 .. 1\n",
              "    -0.5113350519409068    -3.2436187197088e-05           1 .. 0\n",
              "      0.355004333231315 -4.6792409931105244e-05           1 .. 1\n",
              "     -1.077928417176585 -1.2670145630742544e-05           0 .. 0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8jYV84XbP2j",
        "colab_type": "text"
      },
      "source": [
        "### Pregunta 3.1\n",
        "\n",
        "Qué puede observar en esta comparativa?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6Bzp4qIgMPN",
        "colab_type": "text"
      },
      "source": [
        "**En general, los parámetros estimados difieren significativamente entre el modelo de regresión logística y la red neuronal.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4yqpmVobP2l",
        "colab_type": "text"
      },
      "source": [
        "Veamos la exactitud de los modelos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y_mtdy8bP2l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a26ecd80-7e55-4307-e318-9ac8031eb957"
      },
      "source": [
        "print(\"La neurona sigmoide tiene una exactitud de entrenamiento: \" \n",
        "      +str(float((d['Prediccion_entrenamiento'] == CE_y2).mean())) +\" y de validacion: \" +str(float((d['Prediccion_prueba'] == CP_y2).mean())))\n",
        "print(\"La regresion tradicional tiene una exactitud de entrenamiento: \" \n",
        "      +str(float((y_tr == CE_y).mean())) +\" y de validacion: \" +str(float((y_pred == CP_y).mean())))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "La neurona sigmoide tiene una exactitud de entrenamiento: 0.7 y de validacion: 0.7\n",
            "La regresion tradicional tiene una exactitud de entrenamiento: 0.785 y de validacion: 0.7575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDSHahrMbP2q",
        "colab_type": "text"
      },
      "source": [
        "### Ejercicio  3.2\n",
        "\n",
        "Ahora puede desarrollar su propio código intentando mejorar los resultados obtenidos. \n",
        "\n",
        "Intente sobrepasar los resultados de la regresion logistica tradicional. Optimice la tasa de aprendizaje, el número de iteraciones o (bono) investigue y cambie la manera en la cual inicializamos los coeficientes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw-lPm2IZKe1",
        "colab_type": "text"
      },
      "source": [
        "##### Se modifico la funcion que inicializa los pesos y la que ajusta el modelo para generarlos aleatorios, adicionalmente se varian junto con las tasas, y el numero de iteraciones esperando encontrar un accuracy superior. De acuerdo con lo anterior y con lo visto en clase, se recomienda implementar otro algoritmo de estimacion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCKTocPVXrkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import randint, uniform,random"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWirQjMTPMES",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f73238a5-df7b-400b-f6c6-fa1da7a2aef3"
      },
      "source": [
        "## semilla de pesos \n",
        "## tasas\n",
        "## semilla de train prueba\n",
        "## no iteraciones\n",
        "\n",
        "tasas = [1e-4, 1e-6, 1e-10, 2e-20]\n",
        "semillas=[0,1,2,3,4,5,50,51,52,100,1000]\n",
        "n_iteraciones=[15000,20000,25000,30000]\n",
        "for i in semillas:\n",
        "  for j in tasas:\n",
        "    for k in n_iteraciones:\n",
        "    #print (\"La tasa de aprendizaje es: \" + str(i))\n",
        "    #modelos[str(i)] = \n",
        "      #print(semillas[str(i)])\n",
        "      #print(tasas[str(j]))\n",
        "      #print(n_iteraciones[str(k]))\n",
        "      modelo(CE_x2, CP_x2, CE_y2, CP_y2, num_iter = k, tasa = j, print_cost = False,semilla=i)\n",
        "#    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
        "\n",
        "#print(\"el coste es:\")\n",
        "#print(modelos[str(i)][\"Costes\"])\n",
        "\n",
        "#for i in tasas:\n",
        " #   plt.plot(np.squeeze(modelos[str(i)][\"Costes\"]), label= str(modelos[str(i)][\"Tasa de aprendizaje\"]))\n",
        "\n",
        "#plt.ylabel('coste')\n",
        "#plt.xlabel('iteraciones (en cientos)')\n",
        "\n",
        "#legend = plt.legend(loc='upper center', shadow=True)\n",
        "#frame = legend.get_frame()\n",
        "#frame.set_facecolor('0.90')\n",
        "#plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in exp\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 33.16666666666667 %\n",
            "Accuracy de prueba: 33.5 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 67.83333333333334 %\n",
            "Accuracy de prueba: 70.75 %\n",
            "Accuracy de entrenamiento: 67.16666666666667 %\n",
            "Accuracy de prueba: 70.25 %\n",
            "Accuracy de entrenamiento: 67.16666666666667 %\n",
            "Accuracy de prueba: 69.25 %\n",
            "Accuracy de entrenamiento: 66.83333333333334 %\n",
            "Accuracy de prueba: 68.25 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 33.33333333333334 %\n",
            "Accuracy de prueba: 33.5 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.25 %\n",
            "Accuracy de entrenamiento: 66.83333333333334 %\n",
            "Accuracy de prueba: 71.5 %\n",
            "Accuracy de entrenamiento: 66.83333333333334 %\n",
            "Accuracy de prueba: 69.5 %\n",
            "Accuracy de entrenamiento: 66.33333333333334 %\n",
            "Accuracy de prueba: 69.75 %\n",
            "Accuracy de entrenamiento: 66.33333333333334 %\n",
            "Accuracy de prueba: 69.75 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 33.33333333333334 %\n",
            "Accuracy de prueba: 33.5 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.25 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 33.16666666666667 %\n",
            "Accuracy de prueba: 33.5 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 33.33333333333334 %\n",
            "Accuracy de prueba: 33.5 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 70.0 %\n",
            "Accuracy de prueba: 70.0 %\n",
            "Accuracy de entrenamiento: 30.0 %\n",
            "Accuracy de prueba: 30.0 %\n",
            "Accuracy de entrenamiento: 30.33333333333333 %\n",
            "Accuracy de prueba: 30.75 %\n",
            "Accuracy de entrenamiento: 66.83333333333334 %\n",
            "Accuracy de prueba: 70.75 %\n",
            "Accuracy de entrenamiento: 69.5 %\n",
            "Accuracy de prueba: 70.5 %\n",
            "Accuracy de entrenamiento: 30.0 %\n",
            "Accuracy de prueba: 30.0 %\n",
            "Accuracy de entrenamiento: 30.0 %\n",
            "Accuracy de prueba: 30.0 %\n",
            "Accuracy de entrenamiento: 30.0 %\n",
            "Accuracy de prueba: 30.0 %\n",
            "Accuracy de entrenamiento: 30.0 %\n",
            "Accuracy de prueba: 30.0 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-1497db9ba652>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0;31m#print(tasas[str(j]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;31m#print(n_iteraciones[str(k]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mmodelo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCE_x2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCP_x2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCE_y2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCP_y2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msemilla\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;31m#    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-6b39a7184f4f>\u001b[0m in \u001b[0;36mmodelo\u001b[0;34m(CE_x, CP_x, CE_y, CP_y, num_iter, tasa, print_cost, semilla)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Descenso en la dirección del gradiente (GD)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcostes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimiza\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCE_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCE_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Recupere los parámetros w y b del diccionario \"params\" ##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-fafa2600d8c0>\u001b[0m in \u001b[0;36moptimiza\u001b[0;34m(w, b, X, Y, num_iter, tasa, print_cost)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Computación del coste y el gradiente\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoste\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpropaga\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Recupere las derivadas de grads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-25d87034d451>\u001b[0m in \u001b[0;36mpropaga\u001b[0;34m(w, b, X, Y)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m                    \u001b[0;31m# compute la activación\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mcoste\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m                \u001b[0;31m# compute el coste\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TedFquISsXqW",
        "colab_type": "text"
      },
      "source": [
        "**El máximo accuracy obtenido para la red neuronal fue de 71.5% de los datos de prueba, es decir, un incremento de tan solo 1,5 puntos porcentuales sobre el accuracy del modelo inicial.**"
      ]
    }
  ]
}